## 类别型特征处理

字符串形式的特征：

除了**决策树**等少数模型可以直接处理字符型输入，对于**SVM**，**Logistic Regression**等无法处理。

How处理：

1. 编号:label_encoder

2. one-hot:使用稀疏向量表示特征，但要注意特征维度（**维度过多易：1、过拟合风险，2、高维空间基于距离的算法很难衡量；3、部分维度对预测有用**）的特征选择。

3.  二进制编码：利用二进制对ID进行哈希映射

   | 类别ID | 二进制表示 |
   | ------ | ---------- |
   | 1      | 0 0 1      |
   | 2      | 0 1 0      |
   | 3      | 0 1 1      |



## 高维组合特征处理

![img](https://img2020.cnblogs.com/blog/1718532/202011/1718532-20201110164321818-1118953229.png)

![img](https://img2020.cnblogs.com/blog/1718532/202011/1718532-20201110164335346-568521993.png)

特征降维其实从大的方面来讲有两种思路可以走：

- **基于原有的特征进行降维**
- **基于原有的特征进行筛选**

第一种降维方法中，常见的有**：PCA、LDA、SVD、稀疏自编码、word2vec等**

第二种筛选的方法主要是对原有特征和目标label进行**重要性分析**，将那些不重要的特征进行剔除，比如使用gbdt、random forest等模型进行简单的模型训练，并输出特征的权重，继而进行筛选

但有时为了能更有效地找出输入数据内部的结构和模式，会寻找一组超完备基向量，其维度可能比输入的特征维度还要高。

补充一下特征降维的好处：

- 数据维度降低，存储所需的空间减少
- 减少计算和训练模型的时间
- 剔除无用或关系不大的特征，减小对模型的影响，提高模型可用性
- 删除冗余特征（比如某几维特征存在多重共线性）
- 便于数据可视化



## 基于决策树的特征组合方法

并不是所有的特征组合都有意义，需要一种有效的方法帮助我们找到哪些特征需要组合。

例如一个点击预测问题：输入特征有年龄、性别、用户类型(试用期/付费)、物品类型(护肤/食品)。我们构造一个决策树如下：

![img](https://upload-images.jianshu.io/upload_images/15697855-4b0624f862979a8e.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

从根节点到叶节点的每条路径都可以看成一种特征组合的方式。根据上面建立的决策树，我们有4条路径。则可以得到以下样本的编码方式。

| 是否点击 | 年龄 | 性别 | 用户类型 | 物品类型 | 编码      |
| -------- | ---- | ---- | -------- | -------- | --------- |
| 是       | 28   | 女   | 免费     | 护肤     | (1,1,0,0) |
| 否       | 36   | 男   | 付费     | 食品     | (0,0,1,1) |

如第一条样本，满足图上的下面的两条路径，则可编码为(1,1,0,0)。
 注：感觉这种方式确实是组合特征降维的比较好的方式，但是问题是首先要建立一颗树。




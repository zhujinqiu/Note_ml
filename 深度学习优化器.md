目前，最流行并且使用很高的优化器（算法）包括SGD、具有动量的SGD、RMSprop、具有动量的RMSProp、AdaDelta和Adam。在实际应用中，选择哪种优化器应结合具体问题；同时，也优化器的选择也取决于使用者对优化器的熟悉程度

## 一、什么是优化器

深度学习的目标是通过不断改变网络参数，使得参数能够对输入做各种非线性变换拟合输出**，本质上就是一个函数去寻找最优解**，所以如何去更新参数是深度学习研究的重点。通常将更新参数的算法称为优化器，字面理解就是通过什么算法去优化网络模型的参数。常用的优化器就是梯度下降。接下来讲的就是梯度下降和进一步优化梯度下降的各种算法。



## 二、**梯度下降法(Gradient Descent)**

梯度下降算法特别容易理解，**函数的梯度方向表示了函数值增长速度最快的方**向，那么和它相反的方向就可以看作是函数值减少速度最快的方向。对机器学习模型优化的问题，当**目标设定为求解目标函数最小值时**，只要朝着梯度下降的方向前进，就能不断逼近最优值。
根据用多少样本量来更新参数将梯度下降分为三类：BGD，SGD，MBGD

#### （1）**BGD：Batch gradient descent**
每次使用整个数据集计算损失后来更新参数，很显然计算会很慢，占用内存大且不能实时更新，优点是能够收敛到全局最小点，对于异常数据不敏感。
#### （2） SGD:Stochastic gradient descent
这就是常说的随机梯度下降，每次**更新度随机采用一个样本计算损失来更新参数**，计算比较快，占用内存小，可以随时新增样本。这种方式对于样本中的**异常数据敏感**，**损失函数容易震荡**。容易收敛到局部极小值，但由于震荡严重，会跳出局部极小，从而寻找到接近全局最优的解。
#### （3）MBGD: Mini-batch gradient descent
小批量梯度下降，很好理解，将BGD和SGD结合在一起，每次从数据集合中选取一小批数据来计算损失并更新网络参数。

| 用多少样本量来更新参数 | 1                   | 部分                   | 全部                |
| ---------------------- | ------------------- | ---------------------- | ------------------- |
| 梯度下降类型           | 随机梯度下降（SGD） | 小批量梯度下降（MBGD） | 批量梯度下降（BGD） |

```python
improt torch
...
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
...
optimizer.zero_grad()
...
optimizer.step()
```



## 三、动量优化法

动量优化方法是在梯度下降法的基础上进行的改变，具有加速梯度下降的作用。一般有标准动量优化方法Momentum、NAG（Nesterov accelerated gradient）动量优化方法。
NAG在Tensorflow中与Momentum合并在同一函数tf.train.MomentumOptimizer中，可以通过参数配置启用。
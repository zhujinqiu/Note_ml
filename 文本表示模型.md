### 框架

![img](https://img-blog.csdnimg.cn/502305e869154d31802510257224cec8.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oSP5b-15Zue5aSN,size_20,color_FFFFFF,t_70,g_se,x_16)

- 基于one-hot, tf-idf, textrank等的bag-of-words；
-  基于计数的，主题模型，如LSA, pLSA, LDA
-  基于预测的，静态词嵌入，如Word2Vec, FastText, Glove
-  基于大规模预训练的，动态词嵌入，如BERT, ELMo, GPT, T5


### 1、词袋模型

将每篇文章看成一袋子词， 并忽略每个词出现的顺序。 具体地说， 就是将**整段文本**以词为单位切分开，然后**每篇文章**可以表示成一个长向量， 向量中的每一维代表一个单词， 而该维对应的权重则反映了这个词在原文章中的重要程度。**常用TF-IDF来计算权重**。



### 2、TF-IDF

TF-IDF（term frequency–inverse document frequency）是一种用于信息检索与数据挖掘的常用**加权技术**。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200430230735521.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIxMjAxMjY3,size_16,color_FFFFFF,t_70#pic_center)

  TF－IDF的主要思想是：**如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。**某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，**TF-IDF倾向于过滤掉常见的词语，保留重要的词语**。

- **TF** 词频(Term Frequency)

TF，词频，指的是某一个给定的词语在该文件中出现的次数。这个数字通常会被归一化（分子一般小于分母区别于IDF），以防止它偏向长的文件。（**同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否**。）



![img](https://img-blog.csdnimg.cn/3f4c100106b544729ca75f72f01a27c4.png)

- **IDF** 逆文本频率指数(Inverse Document Frequency)

  IDF，逆文本频率指数，是一个词语普遍重要性的度量。某一特定词语的IDF，可以由**总文件数目**除以**包含该词语之文件的数目**，再将得到的商取对数得到。

![img](https://img-blog.csdnimg.cn/7b6f02992bfa453ead0e398c1de6aa43.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5oSP5b-15Zue5aSN,size_10,color_FFFFFF,t_70,g_se,x_16)

### 3、Ngram模型

N-Gram是一种基于统计语言模型的算法。它的基本思想是将**文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列。**

每一个字节片段称为gram，对所有gram的出现频度进行统计，并且按照事先设定好的阈值进行过滤，形成**关键gram列表**，也就是这个文本的向量特征空间，列表中的每一种gram就是一个特征向量维度。

假设：**第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。**



- **应用场景:**

   N-gram 计算分别计算两句话的概率判断那句话更像人话。应用比如搜索引擎的输入猜想。

- **计算方式**

假设有一个由 n 个词组成的句子![img](https://img-blog.csdnimg.cn/02a65ca6f91846aa8573581641230fab.png) ，假设每一个词![img](https://img-blog.csdnimg.cn/4b6018b5699a4da09d04d5729ef51923.png)**都依赖于第一个单词![img](https://img-blog.csdnimg.cn/f4710533ae9d4be189c33b47d459cc9e.png)到它之前一个单词![img](https://img-blog.csdnimg.cn/e42218e895f94df3a0e2c7baec463d1f.png)的影响：**

![img](https://img-blog.csdnimg.cn/7e70a9ae1dfc4cc09fc287f4bbd4cc8a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5oSP5b-15Zue5aSN,size_20,color_FFFFFF,t_70,g_se,x_16)

  但是这个衡量方法有 两个缺陷：1**.参数空间过大 2.数据稀疏严重**

​     为解决第一个问题，引入**马尔可夫假设：**一个词的出现仅与它之前的若干个词有关：

![img](https://img-blog.csdnimg.cn/8f1a1c21f82749afb6a9633917e635c9.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5oSP5b-15Zue5aSN,size_20,color_FFFFFF,t_70,g_se,x_16)

1. **Bi-gram：**如果一个词的出现仅依赖于它前面出现的一个词，就称之为**Bi-gram:**
    ![img](https://img-blog.csdnimg.cn/fe50e277f7194545a23ec3d455d00df0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5oSP5b-15Zue5aSN,size_20,color_FFFFFF,t_70,g_se,x_16)

2. **Tri-gram：**如果一个词的出现仅依赖于它前面出现的两个词，就称之为**Tri-gram**：

   ![img](https://img-blog.csdnimg.cn/f1597640285b447abd86b27bebcb11a2.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5oSP5b-15Zue5aSN,size_20,color_FFFFFF,t_70,g_se,x_16)

- 计算方式，极大似然估计，：
   ![img](https://img-blog.csdnimg.cn/86f2e7884a7f40449e25e8e25feb1552.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5oSP5b-15Zue5aSN,size_20,color_FFFFFF,t_70,g_se,x_16)

#### example

对于一个语料库

![img](https://pic1.zhimg.com/80/v2-43745e5526b6eb8341a896c8abe8c640_1440w.jpg)

已知条件概率：

<img src="https://pic1.zhimg.com/80/v2-a7c0d77143e0c997abd45e1535eaeb8c_1440w.jpg" alt="img" style="zoom:67%;" />

p(want|<s>) = 0.25

下面这个表给出的是基于Bigram模型进行计数之结果

![img](https://pic3.zhimg.com/80/v2-d9d0700496b79f4456776df80856f37a_1440w.jpg)

例如，其中第一行，第二列 表示给定前一个词是 “i” 时，当前词为“want”的情况一共出现了827次。据此，我们便可以算得相应的频率分布表如下。

![img](https://pic2.zhimg.com/80/v2-e53505cae4d3fa21d29f1ff2c267eb49_1440w.jpg)

比如说，我们就以表中的p(eat|i)=0.0036这个概率值讲解，从表一得出“i”一共出现了2533次，而其后出现eat的次数一共有9次，p(eat|i)=p(eat,i)/p(i)=count(i,eat)/count(i)=9/2533 = 0.0036

下面我们通过基于这个语料库来判断s1=“ **i want english food**” 与s2 = "**want i english food**"哪个句子更合理.



P(s1)=P(i|)P(want|i)P(english|want)P(food|english)P(|food)

=0.25×0.33×0.0011×0.5×0.68=0.000031

P(s2)=P(want|<s>)P(i|want)P(english|want)P(food|english)P(</s>|food)

=0.25*0.0022*0.0011*0.5*0.68 = 0.00000002057

0.00000002057<0.000031,也就是说s1= "i want english food"更像人话



### 4、主题模型

主题模型的目标是在大量的文档中自动发现隐含的主题信息

- 传统的方法以单词向量表示文本的语义内容，以单词向量空间的度量表示文本之间的语义相似度
- **潜在语义分析 旨在 解决这种方法不能准确表示语义的问题，试图从大量的文本数据中发现潜在的话题**
- 以话题向量表示文本的语义内容，以话题向量空间的度量更准确地表示文本之间的语义相似度 

#### LSA 

LSA（Latent Semantic Analysis，潜在语义分析）

- 一种无监督学习方法，主要用于文本的话题分析

- 其特点是通过**矩阵分解**发现文本与单词之间的基于话题的语义关系

- 最初应用于`文本信息检索`，也被称为潜在语义索引（latent semantic indexing，LSI），在推荐系统、图像处理、生物信息学等领域也有广泛应用



- 


首先构建**doc-word共现矩阵**，矩阵元素为词的**tf-idf值**（常见词不一定和主题相关，所以tf-idf比词频适用）。

为了找到潜在的语义（主题），LSA利用**奇异值分解SVD**，把**高维的doc-word共现矩阵映射（降维）到低维的潜在语义空间**，得到映射后的文**档向量和词向量**，并且具有相似主题分布的文档（或词）向量接近。

![img](https://pic4.zhimg.com/80/v2-2044c66c00dcec7fa7042e6cf8e30187_1440w.jpg)

设话题数 k ，文档数 n ，单词数 m ，则单词-文档矩阵 Am×n ，且据截断奇异值分解（利用前 k 大特征根以及其对应的特征向量对 A 进行重建）： Am×n≈Um×kDk×kVn×kT ，而单词-话题矩阵为 U ，话题-文档矩阵为 DVT ，下游任务可计算文档间相似度矩阵。



### 5、word2vec

word2vec 是一种将word转为向量的方法，其包含两种算法，分别是**skip-gram**和**CBOW**，它们的最大区别是**skip-gram是通过中心词去预测中心词周围的词**，而**CBOW是通过周围的词去预测中心词**。

![在这里插入图片描述](https://img-blog.csdnimg.cn/2021061621052939.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3ZpbmNlbnRfZHVhbg==,size_16,color_FFFFFF,t_70)

这种将高维度的词表示转换为低维度的词表示的方法，我们称之为词嵌入**（word embedding）**。



![在这里插入图片描述](https://img-blog.csdnimg.cn/20210616211646204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3ZpbmNlbnRfZHVhbg==,size_16,color_FFFFFF,t_70)

1. 当前词的上下文词语的one-hot编码输入（1*V）到输入层。
2. 这些词分别乘以同一个矩阵$W_1$后得到各自的1*N向量，
3. 将这些1*N向量平均为一个1N向量
4. 将$1*N$向量乘矩阵$W_2$，变城一个$1*V$向量
5. $Softmax$归一化后输出每个词的改了向量1*V
6. 概率最大的数对应的词作为预测词
7. 将预测$1*V$和真实标签$1*V$向量计算误差，一啊不能是交叉熵
8. 反向传播误差，不断调整$W_1$和$W_2$矩阵的值y



你可能会想，word2vec不是要将词转为分布式表示的词嵌入么？怎么变成预测中心词了？
其实我们在做CBOW时，最终要的就是$W_1$这个V*N矩阵

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210616215326828.png)
